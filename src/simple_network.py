from typing import Callable
import numpy as np
from src.sigmoid import sigmoid
from src.softmax import softmax
from src.mean_squared_error import mean_squared_error as mse
from src.cross_entropy_error import cross_entropy_error as cee
from src.numerical_gradient import numerical_gradient as num_grad


class SimpleNet():
    '''
    An N layers network.

    '''

    def _construct_weights(self, neurons:dict) -> dict:
        '''
        compose the weights matrices from given neurons 
        '''

        neu_names = list(neurons.keys())
        nm_components = list(zip(neu_names, neu_names[1:]))
        names = [t[0] + '_' + t[1] for t in nm_components ]

        neu_num = list(neurons.values())
        w_dims =  list(zip(neu_num[1:], neu_num))

        weights_meta = dict(zip(names, w_dims))

        # print(f'weights_meta:{weights_meta}')

        return weights_meta

    def __init__(self, neurons:dict):
        '''
        neurons(dict): keys is the name of each layer, value is the number of neurons in each layer.
        '''
        self.weights = {}
        self.biases = {}

        weights_meta = self._construct_weights(neurons)

        # for name, shape in weights_meta.items():
        #     fan_in = shape[1]   #number of input columns
        #     self.weights[name] = np.random.randn(*shape) * np.sqrt(1 / fan_in)
        #     self.biases[name] = np.zeros((shape[0],1))

        for name, shape in weights_meta.items():
            fan_in, fan_out = shape[1], shape[0]
            limit = np.sqrt(6 / (fan_in + fan_out))  # Xavier uniform
            self.weights[name] = np.random.uniform(-limit, limit, shape)
            self.biases[name] = np.zeros((shape[0], 1))
        
    def predict(self, samples):
        '''
        y = weights * x + b
        z = sigmoid(y) if the current weight is not the output weight, otherwise, z = softmax(y)
        '''
        layers = len(self.weights) - 1
        i_layer = 0
        z = samples
        for name in self.weights.keys():
            y = np.dot(self.weights[name], z) + self.biases[name]
            y = np.clip(y, -500, 500)  # Prevent extreme values
            if i_layer == layers:
                z = softmax(y)
                
            else:
                z = sigmoid(y)
                i_layer += 1
        return z
    
    def loss(self, samples:np.ndarray, labels:np.ndarray, cost_func:Callable = None) -> float:
        '''
        calc the loss of the predict generated by the samples against the labels
        cost_func(None): default to cross_entropy_error
        '''
        if cost_func == None:
            cost_func = cee
        
        if not callable(cost_func):
            raise TypeError(f'cost function: {cost_func} is not callable.') 
        
        pred = self.predict(samples)
        return cost_func(pred, labels)

    def accuracy(self, samples:np.ndarray, labels:np.ndarray) -> float:
        '''
        calc the prediction accuracy for the given samples and labels
        '''

        pred = self.predict(samples)

        if pred.shape != labels.shape:
            raise ValueError(f"prediction's shape {pred.shape} mismatches labels' shape {labels.shape}")
        
        return sum(np.argmax(pred, axis=0) == np.argmax(labels, axis=0)) / labels.shape[1] 
    

    def gradients(self, samples: np.ndarray, labels: np.ndarray) -> dict:
        """
        Compute gradients using backpropagation.
        samples: (n_features, n_samples), e.g., (784, 200)
        labels: (n_classes, n_samples), e.g., (10, 200), one-hot
        """

        grads = {'weights': {}, 'biases': {}}
        activations = [samples]  # Z_0 = X
        pre_activations = []     # Y_l values
        N = samples.shape[1]

        out_w_name = list(self.weights.keys())[-1]

        # Forward pass
        a = samples
        for name in self.weights.keys():
            z = np.dot(self.weights[name], a) + self.biases[name]
            z = np.clip(z, -500, 500)
            pre_activations.append(z)
            a = sigmoid(z) if name != out_w_name else softmax(z)
            activations.append(a)

        # Backward pass
        delta = 1.0 / N * (activations[-1] - labels)  # Output layer: Î´^(l+1) = 1/N(A^(l+1) - T)
        for i, name in enumerate(reversed(self.weights.keys())):  # (0, h2_o), (1, h1_h2), (2, i_h1)
            grads['weights'][name] = np.dot(delta, activations[-i-2].T) 
            grads['biases'][name] = np.sum(delta, axis=1, keepdims=True)
            if i < len(self.weights) - 1:  # Hidden layers
                # Use pre-activations for sigmoid derivative
                z = pre_activations[-i-2]  # Z^(l) for this layer
                a = activations[-i-2]
                # a = sigmoid(z)  # Recompute A^(l) (should match activations[-i-2])
                # assert np.allclose(a, activations[-i-2]), f"Mismatch at layer {name}"
                delta = np.dot(self.weights[name].T, delta) * a * (1 - a)

        return grads
    

    def __gradients(self, samples: np.ndarray, labels: np.ndarray) -> dict:
        """
        Compute gradients using backpropagation.
        samples: (n_features, n_samples), e.g., (784, 200)
        labels: (n_classes, n_samples), e.g., (10, 200), one-hot
        """
        grads = {'weights': {}, 'biases': {}}
        activations = [samples]  # Z_0 = X
        pre_activations = []     # Y_l values
        N = samples.shape[1]     # Batch size

        # Forward pass
        z = samples
        for name in self.weights.keys():
            y = np.dot(self.weights[name], z) + self.biases[name]
            pre_activations.append(y)
            z = sigmoid(y) if name != 'h2_o' else softmax(y)
            activations.append(z)

        # Backward pass
        delta = activations[-1] - labels  # Output layer: Z_3 - T
        for i, name in enumerate(reversed(self.weights.keys())):
            grads['weights'][name] = np.dot(delta, activations[-i-2].T) / N
            grads['biases'][name] = np.mean(delta, axis=1, keepdims=True)
            if i < len(self.weights) - 1:  # Hidden layers
                delta = np.dot(self.weights[name].T, delta) * activations[-i-2] * (1 - activations[-i-2])

        return grads

    def _gradients(self, samples:np.ndarray, labels:np.ndarray, grad_func:Callable = None) -> dict:
        '''
        calc the gradient matrices of the weights matrices and the biases matrices, by using grad_func
        returns a dict containing the gradient matrices
        samples(np.ndarray): samples
        labels(np.ndarray): labels
        grad_func(callable): gradient function

        returns:
        grads(dict): {'weights':{}, 'biases':{}}
        '''

        if grad_func == None:
            grad_func = num_grad

        if not callable(grad_func):
            raise TypeError(f"Given gradient function {grad_func} is not callable.")
        
        grads = {
            'weights':{},
            'biases':{}
            }

        # init gradient w and b
        for name, w in self.weights.items():
            grads['weights'][name] = np.zeros(w.shape)
            grads['biases'][name] = np.zeros((w.shape[0],1))

        # loss function
        loss = lambda w: self.loss(samples, labels)
        
        # gradient of Ws    
        for name in self.weights.keys():
            
            grads['weights'][name] = grad_func(loss, self.weights[name])
            # print(f"Grad W {name} max: {np.max(np.abs(grads['weights'][name]))}")
            grads['biases'][name] = grad_func(loss, self.biases[name])
            # print(f"Grad b {name} max: {np.max(np.abs(grads['biases'][name]))}")

        return grads

        